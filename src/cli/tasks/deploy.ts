import path from "path";
import fs from "fs";
import { execa } from "execa";
import {
  LambdaClient,
  CreateFunctionCommand,
  UpdateFunctionCodeCommand,
  UpdateFunctionConfigurationCommand,
  GetFunctionCommand,
  AddPermissionCommand,
  PutFunctionConcurrencyCommand,
  CreateEventSourceMappingCommand,
  ListEventSourceMappingsCommand,
} from "@aws-sdk/client-lambda";
import {
  ECRClient,
  CreateRepositoryCommand,
  DescribeRepositoriesCommand,
} from "@aws-sdk/client-ecr";
import {
  S3Client,
  CreateBucketCommand,
  PutBucketNotificationConfigurationCommand,
  GetBucketNotificationConfigurationCommand,
  HeadBucketCommand,
  PutBucketCorsCommand,
  type LambdaFunctionConfiguration,
  type NotificationConfiguration,
} from "@aws-sdk/client-s3";
import {
  SQSClient,
  CreateQueueCommand,
  GetQueueAttributesCommand,
  SetQueueAttributesCommand,
} from "@aws-sdk/client-sqs";
import {
  IAMClient,
  GetRoleCommand,
  CreateRoleCommand,
  AttachRolePolicyCommand,
  PutRolePolicyCommand,
} from "@aws-sdk/client-iam";
// No IAM client required here; Lambda AddPermission is used for S3 invoke
import type { TransflowConfig } from "../../core/types";
import { computeTmpBucketName } from "../../core/config";
import { bakeTemplates } from "../../core/bake";

interface DeployArgs {
  cfg: TransflowConfig;
  branch: string;
  sha: string;
  tag: string;
  nonInteractive: boolean;
  forceRebuild?: boolean;
}

function imageUri(
  accountId: string,
  region: string,
  repo: string,
  tag: string
) {
  return `${accountId}.dkr.ecr.${region}.amazonaws.com/${repo}:${tag}`;
}

export async function deploy(args: DeployArgs) {
  const { cfg, branch, sha, tag } = args;
  const region = cfg.region;
  const ecr = new ECRClient({ region });
  const lambda = new LambdaClient({ region });
  const s3 = new S3Client({ region });
  const sqs = new SQSClient({ region });
  const iam = new IAMClient({ region });

  async function waitForLambdaUpdate(functionName: string) {
    const start = Date.now();
    const timeoutMs = 5 * 60 * 1000;
    while (true) {
      const { Configuration } = await lambda.send(
        new GetFunctionCommand({ FunctionName: functionName })
      );
      const status = (Configuration as any)?.LastUpdateStatus as
        | "Successful"
        | "InProgress"
        | "Failed"
        | undefined;
      if (status === "Successful") return;
      if (status === "Failed")
        throw new Error(`Lambda update failed for ${functionName}`);
      if (Date.now() - start > timeoutMs) {
        throw new Error(
          `Timeout waiting for Lambda update for ${functionName}`
        );
      }
      await new Promise((r) => setTimeout(r, 5000));
    }
  }

  function sleep(ms: number) {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  // Compute tmp bucket (always created)
  const tmpBucket = computeTmpBucketName(cfg.project, cfg.region);

  // Ensure ECR repo
  try {
    await ecr.send(
      new DescribeRepositoriesCommand({ repositoryNames: [cfg.ecrRepo] })
    );
  } catch {
    await ecr.send(
      new CreateRepositoryCommand({ repositoryName: cfg.ecrRepo })
    );
  }

  // Get AWS account ID
  const { stdout }: { stdout: string } = await execa(
    "aws",
    ["sts", "get-caller-identity", "--query", "Account", "--output", "text"],
    { env: process.env }
  );
  const accountId = stdout.trim();

  // Prepare Docker build context: bake templates and runtime into temp dir
  const contextDir = path.join(process.cwd(), ".transflow-build");
  if (fs.existsSync(contextDir)) {
    fs.rmSync(contextDir, { recursive: true, force: true });
  }
  fs.mkdirSync(contextDir, { recursive: true });

  // Bake templates and handlers into contextDir
  await bakeTemplates({
    templatesDir: path.resolve(process.cwd(), cfg.templatesDir),
    outDir: contextDir,
  });

  // Use the runtime package.json generated by bake; avoid copying repo lockfile to prevent tarball references

  // Docker build & push
  const imgUri = imageUri(accountId, region, cfg.ecrRepo, tag);
  await execa("aws", ["ecr", "get-login-password", "--region", region], {
    stdio: ["ignore", "pipe", "inherit"],
  })
    .then(({ stdout }) =>
      execa(
        "docker",
        [
          "login",
          "--username",
          "AWS",
          "--password-stdin",
          `${accountId}.dkr.ecr.${region}.amazonaws.com`,
        ],
        { input: stdout }
      )
    )
    .then(() => {
      const platformArg =
        cfg.lambda.architecture === "arm64" ? "linux/arm64" : "linux/amd64";
      // Use buildx with --provenance=false to avoid multi-arch index manifests; load into local Docker then push
      return execa(
        "docker",
        [
          "buildx",
          "build",
          "--platform",
          platformArg,
          ...(args.forceRebuild ? ["--no-cache"] : []),
          "--provenance=false",
          "-t",
          imgUri,
          "--load",
          contextDir,
        ],
        { stdio: "inherit" }
      );
    })
    .then(() => execa("docker", ["push", imgUri], { stdio: "inherit" }));

  // Setup SQS queues - shared for all branches (no per-branch suffix by default)
  let queueUrl: string | undefined;
  let progressQueueUrl: string | undefined;
  let dlqUrl: string | undefined;

  // SQS is required - use shared names if provided
  const queueName = cfg.sqs.queueName || `${cfg.project}-processing.fifo`;
  // Progress queue removed (DynamoDB is the status store)
  const dlqName = `${cfg.project}-dlq.fifo`;

  // Create DLQ first
  try {
    const dlqResult = await sqs.send(
      new CreateQueueCommand({
        QueueName: dlqName,
        Attributes: {
          FifoQueue: "true",
          ContentBasedDeduplication: "true",
          MessageRetentionPeriod: "1209600", // 14 days
        },
      })
    );
    dlqUrl = dlqResult.QueueUrl;
  } catch (error: any) {
    if (error.name !== "QueueAlreadyExists") throw error;
    // Get existing DLQ URL
    const attrs = await sqs.send(
      new GetQueueAttributesCommand({
        QueueUrl: `https://sqs.${region}.amazonaws.com/${accountId}/${dlqName}`,
        AttributeNames: ["QueueArn"],
      })
    );
    dlqUrl = `https://sqs.${region}.amazonaws.com/${accountId}/${dlqName}`;
  }

  // Create main queue with DLQ
  try {
    const queueResult = await sqs.send(
      new CreateQueueCommand({
        QueueName: queueName,
        Attributes: {
          FifoQueue: "true",
          ContentBasedDeduplication: "true",
          VisibilityTimeout: cfg.sqs.visibilityTimeoutSec?.toString() || "960",
          RedrivePolicy: JSON.stringify({
            deadLetterTargetArn: `arn:aws:sqs:${region}:${accountId}:${dlqName}`,
            maxReceiveCount: cfg.sqs.maxReceiveCount || 3,
          }),
        },
      })
    );
    queueUrl = queueResult.QueueUrl;
  } catch (error: any) {
    if (error.name !== "QueueAlreadyExists") throw error;
    // Get existing queue URL
    queueUrl = `https://sqs.${region}.amazonaws.com/${accountId}/${queueName}`;
  }

  // Progress queue removed; DynamoDB is the source of truth
  progressQueueUrl = undefined;

  // Ensure execution role (create if missing) and attach required permissions
  const providedRoleArn =
    process.env.TRANSFLOW_LAMBDA_ROLE_ARN || cfg.lambda.roleArn || "";
  let roleName: string;
  let executionRoleArn: string;
  if (providedRoleArn) {
    roleName = providedRoleArn.split("/").pop() as string;
    executionRoleArn = providedRoleArn;
    try {
      await iam.send(new GetRoleCommand({ RoleName: roleName }));
    } catch {
      await iam.send(
        new CreateRoleCommand({
          RoleName: roleName,
          AssumeRolePolicyDocument: JSON.stringify({
            Version: "2012-10-17",
            Statement: [
              {
                Effect: "Allow",
                Principal: { Service: "lambda.amazonaws.com" },
                Action: "sts:AssumeRole",
              },
            ],
          }),
          Description: `Transflow execution role for project ${cfg.project}`,
          Tags: [
            { Key: "Project", Value: cfg.project },
            { Key: "Component", Value: "transflow" },
          ],
        })
      );
    }
  } else {
    roleName = `${cfg.project}-transflow-lambda-role`;
    executionRoleArn = `arn:aws:iam::${accountId}:role/${roleName}`;
    try {
      await iam.send(new GetRoleCommand({ RoleName: roleName }));
    } catch {
      await iam.send(
        new CreateRoleCommand({
          RoleName: roleName,
          AssumeRolePolicyDocument: JSON.stringify({
            Version: "2012-10-17",
            Statement: [
              {
                Effect: "Allow",
                Principal: { Service: "lambda.amazonaws.com" },
                Action: "sts:AssumeRole",
              },
            ],
          }),
          Description: `Transflow execution role for project ${cfg.project}`,
          Tags: [
            { Key: "Project", Value: cfg.project },
            { Key: "Component", Value: "transflow" },
          ],
        })
      );
    }
  }

  // Attach basic logging policy (idempotent)
  await iam.send(
    new AttachRolePolicyCommand({
      RoleName: roleName,
      PolicyArn:
        "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole",
    })
  );

  // Build inline policy granting least-privilege for SQS/S3/DynamoDB
  const queueArn = `arn:aws:sqs:${region}:${accountId}:${queueName}`;
  const s3ResourceArns: string[] = [];
  const allBuckets = [
    computeTmpBucketName(cfg.project, cfg.region),
    ...(cfg.s3.exportBuckets || []),
  ];
  for (const b of allBuckets) {
    s3ResourceArns.push(`arn:aws:s3:::${b}`);
    s3ResourceArns.push(`arn:aws:s3:::${b}/*`);
  }
  const ddbTableArn = `arn:aws:dynamodb:${region}:${accountId}:table/${cfg.dynamoDb.tableName}`;
  const inlinePolicy = {
    Version: "2012-10-17",
    Statement: [
      {
        Sid: "SqsAccess",
        Effect: "Allow",
        Action: [
          "sqs:ReceiveMessage",
          "sqs:DeleteMessage",
          "sqs:GetQueueAttributes",
          "sqs:ChangeMessageVisibility",
          "sqs:SendMessage",
        ],
        Resource: queueArn,
      },
      {
        Sid: "S3Access",
        Effect: "Allow",
        Action: [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket",
        ],
        Resource: s3ResourceArns,
      },
      {
        Sid: "DynamoDbStatus",
        Effect: "Allow",
        Action: [
          "dynamodb:PutItem",
          "dynamodb:UpdateItem",
          "dynamodb:GetItem",
          "dynamodb:DescribeTable",
        ],
        Resource: ddbTableArn,
      },
      {
        Sid: "LogsCreate",
        Effect: "Allow",
        Action: [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents",
        ],
        Resource: "*",
      },
    ],
  };
  await iam.send(
    new PutRolePolicyCommand({
      RoleName: roleName,
      PolicyName: "transflow-inline",
      PolicyDocument: JSON.stringify(inlinePolicy),
    })
  );

  // Create or update main processing Lambda
  const functionName = `${cfg.lambdaPrefix}${branch}`;
  const imageConfig = { ImageUri: imgUri };
  let exists = true;
  try {
    await lambda.send(new GetFunctionCommand({ FunctionName: functionName }));
  } catch {
    exists = false;
  }

  if (!exists) {
    const envVars: Record<string, string> = {
      TRANSFLOW_BRANCH: branch,
      MAX_BATCH_SIZE: cfg.lambda.maxBatchSize?.toString() || "10",
    };

    if (queueUrl) {
      envVars["SQS_QUEUE_URL"] = queueUrl;
    }
    // No progress queue env
    // Allowed export buckets and tmp bucket
    envVars["TRANSFLOW_ALLOWED_BUCKETS"] = JSON.stringify(
      cfg.s3.exportBuckets || []
    );
    envVars["TRANSFLOW_TMP_BUCKET"] = tmpBucket;
    envVars["DYNAMODB_TABLE"] = cfg.dynamoDb.tableName;
    envVars["TRANSFLOW_PROJECT"] = cfg.project;
    await lambda.send(
      new CreateFunctionCommand({
        FunctionName: functionName,
        PackageType: "Image",
        Code: imageConfig,
        Role: (process.env.TRANSFLOW_LAMBDA_ROLE_ARN ||
          cfg.lambda.roleArn ||
          executionRoleArn)!,
        MemorySize: cfg.lambda.memoryMb,
        Timeout: cfg.lambda.timeoutSec,
        Architectures: cfg.lambda.architecture
          ? [cfg.lambda.architecture]
          : undefined,
        Environment: { Variables: envVars },
      })
    );
    await waitForLambdaUpdate(functionName);
  } else {
    await lambda.send(
      new UpdateFunctionCodeCommand({
        FunctionName: functionName,
        ImageUri: imgUri,
      })
    );
    await waitForLambdaUpdate(functionName);
    const envVars: Record<string, string> = {
      TRANSFLOW_BRANCH: branch,
      MAX_BATCH_SIZE: cfg.lambda.maxBatchSize?.toString() || "10",
    };

    if (queueUrl) {
      envVars["SQS_QUEUE_URL"] = queueUrl;
    }
    // No progress queue env
    // Allowed export buckets and tmp bucket
    envVars["TRANSFLOW_ALLOWED_BUCKETS"] = JSON.stringify(
      cfg.s3.exportBuckets || []
    );
    envVars["TRANSFLOW_TMP_BUCKET"] = tmpBucket;
    envVars["DYNAMODB_TABLE"] = cfg.dynamoDb.tableName;
    envVars["TRANSFLOW_PROJECT"] = cfg.project;
    await lambda.send(
      new UpdateFunctionConfigurationCommand({
        FunctionName: functionName,
        Role:
          process.env.TRANSFLOW_LAMBDA_ROLE_ARN ||
          cfg.lambda.roleArn ||
          executionRoleArn,
        MemorySize: cfg.lambda.memoryMb,
        Timeout: cfg.lambda.timeoutSec,
        Environment: { Variables: envVars },
      })
    );
    await waitForLambdaUpdate(functionName);
  }

  // Set reserved concurrency if specified
  if (cfg.lambda.reservedConcurrency) {
    try {
      await lambda.send(
        new PutFunctionConcurrencyCommand({
          FunctionName: functionName,
          ReservedConcurrentExecutions: cfg.lambda.reservedConcurrency,
        })
      );
    } catch (error) {
      console.warn(`Failed to set reserved concurrency: ${error}`);
    }
  }

  // Create SQS event source mapping - SQS is now required
  if (queueUrl) {
    const queueArn = `arn:aws:sqs:${region}:${accountId}:${queueName}`;

    // Check if event source mapping already exists
    const existingMappings = await lambda.send(
      new ListEventSourceMappingsCommand({
        FunctionName: functionName,
        EventSourceArn: queueArn,
      })
    );

    if (existingMappings.EventSourceMappings?.length === 0) {
      const maxAttempts = 6;
      for (let attempt = 1; attempt <= maxAttempts; attempt++) {
        try {
          await lambda.send(
            new CreateEventSourceMappingCommand({
              FunctionName: functionName,
              EventSourceArn: queueArn,
              BatchSize: cfg.sqs.batchSize || 10, // Reduce latency
            })
          );
          break;
        } catch (err: any) {
          const msg = String(err?.message || err);
          const isPerm =
            msg.includes("ReceiveMessage") ||
            err?.name === "InvalidParameterValueException";
          if (attempt < maxAttempts && isPerm) {
            const waitMs = 5000 * attempt;
            console.warn(
              `Waiting for IAM policy propagation before creating SQS mapping (attempt ${attempt}/${maxAttempts})...`
            );
            await sleep(waitMs);
            continue;
          }
          throw err;
        }
      }
    }
  }

  // Separate bridge Lambda removed; main Lambda enqueues S3 events into SQS

  // S3 setup: Create tmp bucket + export buckets and NEVER delete
  // Ensure DynamoDB table exists
  try {
    await execa(
      "aws",
      [
        "dynamodb",
        "describe-table",
        "--table-name",
        cfg.dynamoDb.tableName,
        "--region",
        region,
      ],
      { stdio: "ignore" }
    );
  } catch {
    await execa(
      "aws",
      [
        "dynamodb",
        "create-table",
        "--table-name",
        cfg.dynamoDb.tableName,
        "--attribute-definitions",
        "AttributeName=assembly_id,AttributeType=S",
        "--key-schema",
        "AttributeName=assembly_id,KeyType=HASH",
        "--billing-mode",
        "PAY_PER_REQUEST",
        "--region",
        region,
      ],
      { stdio: "inherit" }
    );
  }

  const bucketsToEnsure = [tmpBucket, ...(cfg.s3.exportBuckets || [])];
  if (bucketsToEnsure.length > 0) {
    for (const bucket of bucketsToEnsure) {
      try {
        await s3.send(new HeadBucketCommand({ Bucket: bucket }));
      } catch {
        const params: any = { Bucket: bucket };
        if (region !== "us-east-1")
          params.CreateBucketConfiguration = { LocationConstraint: region };
        await s3.send(new CreateBucketCommand(params));
      }
    }
  }
  // Ensure permissive CORS on tmp bucket for browser uploads (PUT preflight)
  try {
    await s3.send(
      new PutBucketCorsCommand({
        Bucket: tmpBucket,
        CORSConfiguration: {
          CORSRules: [
            {
              AllowedMethods: ["PUT", "POST", "GET", "HEAD"],
              AllowedOrigins: ["*"],
              AllowedHeaders: ["*"],
              ExposeHeaders: ["ETag"],
              MaxAgeSeconds: 3000,
            },
          ],
        },
      })
    );
  } catch (err) {
    console.warn(`Failed to set CORS on tmp bucket ${tmpBucket}: ${err}`);
  }
  // Add PutBucketNotificationConfiguration scoped to uploads/ prefix on tmp bucket only
  for (const bucket of [tmpBucket]) {
    const prefix = `uploads/`;
    const targetFunctionName = functionName;
    const functionArn = `arn:aws:lambda:${region}:${accountId}:function:${targetFunctionName}`;

    // Ensure S3 can invoke Lambda before configuring S3 notifications
    try {
      await lambda.send(
        new AddPermissionCommand({
          FunctionName: targetFunctionName,
          Action: "lambda:InvokeFunction",
          Principal: "s3.amazonaws.com",
          StatementId: `s3-invoke-${branch}-${bucket}`,
          SourceArn: `arn:aws:s3:::${bucket}`,
        })
      );
    } catch {}

    // Replace bucket notification configuration with a single, unambiguous rule we manage
    const notif: NotificationConfiguration = {
      LambdaFunctionConfigurations: [
        {
          Events: ["s3:ObjectCreated:*"],
          LambdaFunctionArn: functionArn,
          Filter: { Key: { FilterRules: [{ Name: "prefix", Value: prefix }] } },
        },
      ],
    };
    await s3.send(
      new PutBucketNotificationConfigurationCommand({
        Bucket: bucket,
        NotificationConfiguration: notif,
      })
    );
  }

  // Always deploy status Lambda with predictable name
  const statusFunctionName = `${cfg.project}-status`;

  console.log(`🚀 Deploying status Lambda: ${statusFunctionName}`);

  let statusExists = true;
  try {
    await lambda.send(
      new GetFunctionCommand({ FunctionName: statusFunctionName })
    );
  } catch {
    statusExists = false;
  }

  const statusEnvVars: Record<string, string> = {
    DYNAMODB_TABLE: cfg.dynamoDb.tableName,
    TRANSFLOW_PROJECT: cfg.project,
  };

  if (!statusExists) {
    // Create status Lambda function with fixed lightweight config
    await lambda.send(
      new CreateFunctionCommand({
        FunctionName: statusFunctionName,
        PackageType: "Image",
        Code: imageConfig,
        Role: (process.env.TRANSFLOW_LAMBDA_ROLE_ARN ||
          cfg.lambda.roleArn ||
          executionRoleArn)!,
        MemorySize: 512, // Fixed lightweight config
        Timeout: 30, // Fixed 30s timeout
        Architectures: cfg.lambda.architecture
          ? [cfg.lambda.architecture]
          : undefined,
        Environment: { Variables: statusEnvVars },
        Description: `Transflow status checker for ${cfg.project}`,
        Tags: {
          Project: cfg.project,
          Component: "status-lambda",
        },
      })
    );
    await waitForLambdaUpdate(statusFunctionName);
    console.log(`✅ Created status Lambda: ${statusFunctionName}`);
  } else {
    // Update existing status Lambda
    await lambda.send(
      new UpdateFunctionCodeCommand({
        FunctionName: statusFunctionName,
        ImageUri: imgUri,
      })
    );
    await waitForLambdaUpdate(statusFunctionName);

    await lambda.send(
      new UpdateFunctionConfigurationCommand({
        FunctionName: statusFunctionName,
        Role:
          process.env.TRANSFLOW_LAMBDA_ROLE_ARN ||
          cfg.lambda.roleArn ||
          executionRoleArn,
        MemorySize: 512, // Fixed lightweight config
        Timeout: 30, // Fixed 30s timeout
        Environment: { Variables: statusEnvVars },
      })
    );
    await waitForLambdaUpdate(statusFunctionName);
    console.log(`✅ Updated status Lambda: ${statusFunctionName}`);
  }

  return {
    imageUri: imgUri,
    functionName,
    statusFunctionName, // Always deployed
  };
}
